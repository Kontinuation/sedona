# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

name: Python build

on:
  push:
    branches:
      - master
    paths:
      - 'common/**'
      - 'spark/**'
      - 'spark-shaded/**'
      - 'pom.xml'
      - 'python/**'
      - '.github/workflows/python.yml'
  pull_request:
    branches:
      - '*'
    paths:
      - 'common/**'
      - 'spark/**'
      - 'spark-shaded/**'
      - 'pom.xml'
      - 'python/**'
      - '.github/workflows/python.yml'

env:
  MAVEN_OPTS: -Dmaven.wagon.httpconnectionManager.ttlSeconds=60
  JAI_CORE_VERSION: '1.1.3'
  JAI_CODEC_VERSION: '1.1.3'
  JAI_IMAGEIO_VERSION: '1.1'
  DO_NOT_TRACK: true
  SPARK_LOCAL_IP: 127.0.0.1

permissions:
  contents: read

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: ${{ github.ref != 'refs/heads/master' }}

jobs:
  build:
    runs-on: ubuntu-22.04
    strategy:
      matrix:
        include:
          - spark: '4.0.0'
            scala: '2.13.8'
            java: '17'
            python: '3.10'
          - spark: '3.5.0'
            scala: '2.12.8'
            java: '11'
            python: '3.10'
            shapely: '1'
          - spark: '3.5.0'
            scala: '2.12.8'
            java: '11'
            python: '3.10'
          - spark: '3.5.0'
            scala: '2.12.8'
            java: '11'
            python: '3.9'
          - spark: '3.5.0'
            scala: '2.12.8'
            java: '11'
            python: '3.8'
          - spark: '3.4.0'
            scala: '2.12.8'
            java: '11'
            python: '3.10'
          - spark: '3.4.0'
            scala: '2.12.8'
            java: '11'
            python: '3.9'
          - spark: '3.4.0'
            scala: '2.12.8'
            java: '11'
            python: '3.8'
          - spark: '3.4.0'
            scala: '2.12.8'
            java: '11'
            python: '3.8'
            shapely: '1'

    env:
      VENV_PATH: /home/runner/.local/share/virtualenvs/python-${{ matrix.python }}
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v4
        with:
          distribution: 'zulu'
          java-version: '${{ matrix.java }}'
      - uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python }}
      - name: Cache Maven packages
        uses: actions/cache@v4
        with:
          path: ~/.m2
          key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
          restore-keys: ${{ runner.os }}-m2
      - env:
          SPARK_VERSION: ${{ matrix.spark }}
          SCALA_VERSION: ${{ matrix.scala }}
        run: |
          SPARK_COMPAT_VERSION=${SPARK_VERSION:0:3}
          mvn -q clean install -DskipTests -Dspark=${SPARK_COMPAT_VERSION} -Dscala=${SCALA_VERSION:0:4} -Dgeotools
      - run: sudo apt-get -y install python3-pip python-dev-is-python3
      - run: sudo pip3 install -U setuptools
      - run: sudo pip3 install -U wheel
      - run: sudo pip3 install -U virtualenvwrapper
      - run: python3 -m pip install uv
      - run: cd python; python3 setup.py build_ext --inplace
      - env:
          SPARK_VERSION: ${{ matrix.spark }}
          PYTHON_VERSION: ${{ matrix.python }}
          SHAPELY_VERSION: ${{ matrix.shapely }}
          PANDAS_VERSION: ${{ matrix.pandas }}
        run: |
          cd python
          # Base dev install
          uv pip install -e .[dev]
          # Conditional shapely version adjustments
          if [ "${SHAPELY_VERSION}" == "1" ]; then
            uv pip install "shapely<2.0.0" --reinstall
          fi
          # Conditional pandas version adjustments
          if [ "${PANDAS_VERSION}" == "1" ]; then
            uv pip install "pandas<2.0.0" --reinstall
          fi
          # Install specific pyspark version matching matrix
          uv pip install pyspark==${SPARK_VERSION} --reinstall
          python -c "import pyspark,sys;print('Using pyspark', pyspark.__version__)"
      - env:
          PYTHON_VERSION: ${{ matrix.python }}
        run: |
          wget --retry-connrefused --waitretry=10 --read-timeout=20 --timeout=15 --tries=5 https://repo.osgeo.org/repository/release/javax/media/jai_core/${JAI_CORE_VERSION}/jai_core-${JAI_CORE_VERSION}.jar
          wget --retry-connrefused --waitretry=10 --read-timeout=20 --timeout=15 --tries=5 https://repo.osgeo.org/repository/release/javax/media/jai_codec/${JAI_CODEC_VERSION}/jai_codec-${JAI_CODEC_VERSION}.jar
          wget --retry-connrefused --waitretry=10 --read-timeout=20 --timeout=15 --tries=5 https://repo.osgeo.org/repository/release/javax/media/jai_imageio/${JAI_IMAGEIO_VERSION}/jai_imageio-${JAI_IMAGEIO_VERSION}.jar
          PY_SITE=$(python - <<'PY'
import site,sys
print(site.getsitepackages()[0])
PY
)
          echo "Python site-packages: $PY_SITE"
          mv -v jai_core-${JAI_CORE_VERSION}.jar ${PY_SITE}/pyspark/jars
          mv -v jai_codec-${JAI_CODEC_VERSION}.jar ${PY_SITE}/pyspark/jars
          mv -v jai_imageio-${JAI_IMAGEIO_VERSION}.jar ${PY_SITE}/pyspark/jars
      - env:
          PYTHON_VERSION: ${{ matrix.python }}
        run: |
          PY_SITE=$(python - <<'PY'
import site
print(site.getsitepackages()[0])
PY
)
          find spark-shaded/target -name sedona-*.jar -exec cp {} ${PY_SITE}/pyspark/jars/ \;
      - name: Run tests
        run: |
          export SPARK_HOME=$(python - <<'PY'
import site
print(site.getsitepackages()[0]+"/pyspark")
PY
)
          cd python
          pytest -v tests
      - name: Run basic tests without rasterio
        run: |
          export SPARK_HOME=$(python - <<'PY'
import site
print(site.getsitepackages()[0]+"/pyspark")
PY
)
          cd python
          uv pip uninstall -y rasterio || true
          pytest -v tests/core/test_rdd.py tests/sql/test_dataframe_api.py
      - name: Run Spark Connect tests
        if: ${{ matrix.spark >= '3.4.0' }}
        run: |
          export SPARK_REMOTE=local
          export SPARK_HOME=$(python - <<'PY'
import site
print(site.getsitepackages()[0]+"/pyspark")
PY
)
          cd python
          uv pip install "pyspark[connect]==${{ matrix.spark }}" --reinstall
          pytest -v tests/sql/test_dataframe_api.py
